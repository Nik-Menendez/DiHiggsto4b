{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uproot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Activation, Dense, Convolution2D, MaxPooling2D, Dropout, Flatten, LeakyReLU\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... opening input files\n",
      "Opening file:  ../analysis/objects_gg_HH_bbbb_SM.root\n",
      "Opening file:  ../analysis/objects_TT_TuneCUETP8M2T4_13TeV-powheg-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT700to1000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT1000to1500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT1500to2000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT500to700_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT2000toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT300to500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT200to300_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../analysis/objects_QCD_HT100to200_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root\n",
      "Opening file:  ../background/data_3btag_with_weights_AR.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m13.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m3.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p10.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p19.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p9.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m500.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m14.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m4.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p11.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p2.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m600.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m15.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m5.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p12.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p20.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m650.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m16.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m6.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p13.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p3.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m260.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m800.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m17.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m7.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p14.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p4.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m270.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m900.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m1.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m18.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m8.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p15.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p5.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m300.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m10.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m19.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m9.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p16.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p6.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m350.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m11.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m2.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p0.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p17.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p7.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m400.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m12.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_m20.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p1.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p18.root\n",
      "Opening file:  ../analysis/objects_GGHH4B_rew_kl_p8.root\n",
      "Opening file:  ../analysis/objects_gg_Xradion_HH_bbbb_m450.root\n"
     ]
    }
   ],
   "source": [
    "# Open signal and background datasets\n",
    "\n",
    "first_bkg   = '../analysis/objects_TT_TuneCUETP8M2T4_13TeV-powheg-pythia8.root'\n",
    "input_bkg   = [ '../analysis/objects_QCD_HT700to1000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT1000to1500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT1500to2000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT500to700_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT2000toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT300to500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT200to300_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../analysis/objects_QCD_HT100to200_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.root',\n",
    "                '../background/data_3btag_with_weights_AR.root']\n",
    "first_sig  = '../analysis/objects_gg_HH_bbbb_SM.root'\n",
    "input_sig  = ['../analysis/objects_GGHH4B_rew_kl_m13.root',  \n",
    "              '../analysis/objects_GGHH4B_rew_kl_m3.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p10.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p19.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p9.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m500.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m14.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m4.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p11.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p2.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m600.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m15.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m5.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p12.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p20.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m650.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m16.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m6.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p13.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p3.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m260.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m800.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m17.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m7.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p14.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p4.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m270.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m900.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m1.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m18.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m8.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p15.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p5.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m300.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m10.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m19.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m9.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p16.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p6.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m350.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m11.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m2.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p0.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p17.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p7.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m400.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m12.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_m20.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p1.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p18.root',\n",
    "              '../analysis/objects_GGHH4B_rew_kl_p8.root',\n",
    "              '../analysis/objects_gg_Xradion_HH_bbbb_m450.root']\n",
    "datsp_bkg  = '../background/data_3btag_with_weights_AR.root'\n",
    "treename = 'bbbbTree'\n",
    "\n",
    "## convert to dataframes\n",
    "vars_training = [ 'H1_b1_pt', 'H1_b2_pt', 'H2_b1_pt', 'H2_b2_pt',\n",
    "#                  'H1_b1_m', 'H1_b2_m', 'H2_b1_m', 'H2_b2_m', \n",
    "                  'H1_b1_eta', 'H1_b2_eta', 'H2_b1_eta', 'H2_b2_eta', \n",
    "                  'H1_eta', 'H1_pt', 'H2_eta', 'H2_pt', \n",
    "                  'HH_eta', 'HH_pt','HH_m', 'H1H2_deltaEta', 'H1_costhetaCM', \n",
    "                  'H1H2_deltaPhi', 'H1_m', 'H2_m']\n",
    "\n",
    "# extra variables needed for preselections\n",
    "all_vars = vars_training + ['n_btag']\n",
    "all_vars = list(set(all_vars))\n",
    "\n",
    "print \"... opening input files\"\n",
    "\n",
    "\n",
    "arrs_bkg = [None]*(len(input_bkg)+1)\n",
    "arrs_sig = [None]*(len(input_sig)+1)\n",
    "\n",
    "print \"Opening file: \", first_sig\n",
    "arrs_sig[0]  = uproot.open(first_sig)[treename]\n",
    "\n",
    "print \"Opening file: \", first_bkg\n",
    "arrs_bkg[0]  = uproot.open(first_bkg)[treename]\n",
    "\n",
    "data_bkg = arrs_bkg[0].pandas.df(all_vars)\n",
    "data_sig = arrs_sig[0].pandas.df(all_vars)\n",
    "\n",
    "j=1\n",
    "for i in (input_bkg):\n",
    "    print \"Opening file: \", i\n",
    "    arrs_bkg[j] = uproot.open(i)[treename]\n",
    "    #temp_bkg = arrs_bkg[j].pandas.df(all_vars)\n",
    "    data_bkg = data_bkg.append(arrs_bkg[j].pandas.df(all_vars))\n",
    "    j=j+1\n",
    "\n",
    "k=1\n",
    "for i in (input_sig):\n",
    "    print \"Opening file: \", i\n",
    "    arrs_sig[k] = uproot.open(i)[treename]\n",
    "    #temp_sig = arrs_bkg[j].pandas.df(all_vars)\n",
    "    data_sig = data_sig.append(arrs_sig[k].pandas.df(all_vars))\n",
    "    k=k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... preselecting data\n"
     ]
    }
   ],
   "source": [
    "print \"... preselecting data\"\n",
    "\n",
    "## apply a selection on the datasets\n",
    "data_bkg = data_bkg[data_bkg['n_btag'] == 3]\n",
    "data_sig = data_sig[data_sig['n_btag'] >= 4]\n",
    "\n",
    "# restrict training to the signal region\n",
    "# data_bkg['chi'] = np.sqrt( (data_bkg['H1_m']-120)*(data_bkg['H1_m']-120)+(data_bkg['H2_m']-110)*(data_bkg['H2_m']-110))\n",
    "# data_sig['chi'] = np.sqrt( (data_sig['H1_m']-120)*(data_sig['H1_m']-120)+(data_sig['H2_m']-110)*(data_sig['H2_m']-110))\n",
    "\n",
    "# data_bkg = data_bkg[data_bkg['chi'] < 30]\n",
    "# data_sig = data_sig[data_sig['chi'] < 30]\n",
    "\n",
    "data_bkg = data_bkg.drop(columns=['n_btag'])#,'H1_m','H2_m','chi'])\n",
    "data_sig = data_sig.drop(columns=['n_btag'])#,'H1_m','H2_m','chi'])\n",
    "\n",
    "## for the signal, add a fake weight column\n",
    "data_bkg['train_w'] = 1#data_bkg['bkg_model_w']\n",
    "# data_bkg.drop('bkg_model_w', axis=1, inplace=True)\n",
    "data_sig['train_w'] = 1\n",
    "    \n",
    "# normalise the sum of weights to unity\n",
    "data_bkg['train_w'] = data_bkg['train_w'].multiply(1./data_bkg['train_w'].sum())\n",
    "data_sig['train_w'] = data_sig['train_w'].multiply(1./data_sig['train_w'].sum())\n",
    "    \n",
    "## label the datasets as bkg (0) and signal (1)\n",
    "data_bkg['isSignal'] = np.zeros(len(data_bkg))\n",
    "data_sig['isSignal'] = np.ones(len(data_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5948c050d73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# preprocessing: standard scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvars_training\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvars_training\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvars_training\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mall_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123456\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc7_amd64_gcc700/external/py2-pandas/0.23.0/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc7_amd64_gcc700/external/py2-pandas/0.23.0/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3139\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3141\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc7_amd64_gcc700/external/py2-pandas/0.23.0/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    586\u001b[0m                     \u001b[0;31m# note that this coerces the dtype if we are mixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0;31m# GH 7551\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                         raise ValueError('Must have equal len keys and value '\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Divide data into testing and training\n",
    "\n",
    "NDIM = len(vars_training)\n",
    "NDIM = len(vars_training)\n",
    "\n",
    "all_data = pd.concat([data_bkg, data_sig], axis=0, sort=False)\n",
    "#normalized_data = (all_data-all_data.min())/(all_data.max()-all_data.min())\n",
    "\n",
    "# preprocessing: standard scalar\n",
    "scaler = StandardScaler().fit(all_data[vars_training])\n",
    "all_data[vars_training] = scaler.transform(all_data[vars_training])\n",
    "\n",
    "all_data_train, all_data_test = train_test_split(all_data, test_size=0.33, random_state=123456)\n",
    "\n",
    "# early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into dense_model.h5\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model_checkpoint = ModelCheckpoint('dense_model.h5', monitor='val_loss', \n",
    "                                  verbose=0, save_best_only=True, \n",
    "                                  save_weights_only=False, mode='auto', \n",
    "                                  period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "\n",
    "inputs = Input(shape=(NDIM,), name = 'input')\n",
    "x = Dropout(.2, name = 'input_dropout_.2')(inputs)\n",
    "x = Dense(128, name = 'hidden1')(x)\n",
    "x = LeakyReLU(.1, name = 'LeakyReLU1_.1')(x)\n",
    "x = Dense(128, name = 'hidden2')(x)\n",
    "x = LeakyReLU(.1, name = 'LeakyReLU2_.1')(x)\n",
    "x = Dropout(.4, name = 'hidden2_dropout_.5')(x)\n",
    "# x = Dense(128, name = 'hidden3')(x)\n",
    "# x = LeakyReLU(.1 , name = 'LeakyReLU3_.1')(x)\n",
    "# x = Dropout(.2, name = 'hidden3_dropout_.2')(x)\n",
    "outputs = Dense(1, name = 'output', kernel_initializer='normal', activation='sigmoid')(x)\n",
    "\n",
    "# creae the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(all_data_train[vars_training], \n",
    "                    all_data_train['isSignal'],\n",
    "                    epochs=50, \n",
    "                    batch_size=1024, \n",
    "                    sample_weight = all_data_train['train_w'].values,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping,model_checkpoint], \n",
    "                    validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot loss vs epoch\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val_loss')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(history.history['acc'], label='acc')\n",
    "ax.plot(history.history['val_acc'], label='val_acc')\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "# Plot ROC\n",
    "Y_predict = model.predict(all_data_test[vars_training])\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(all_data_test['isSignal'], Y_predict)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel('false positive rate')\n",
    "ax.set_ylabel('true positive rate')\n",
    "ax.set_title('receiver operating curve')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Classifier Output\n",
    "\n",
    "def plot_classifier_output():\n",
    "\n",
    "    print '... drawing discriminator output'\n",
    "    \n",
    "    sig_train = all_data_train[all_data_train.isSignal == 1][vars_training]\n",
    "    bkg_train = all_data_train[all_data_train.isSignal == 0][vars_training]\n",
    "    \n",
    "    sig_test  = all_data_test[all_data_test.isSignal == 1][vars_training]\n",
    "    bkg_test  = all_data_test[all_data_test.isSignal == 0][vars_training]\n",
    "\n",
    "    ## these are already normalised to unity\n",
    "    weights_sig_train = all_data_train[all_data_train.isSignal == 1]['train_w']\n",
    "    weights_bkg_train = all_data_train[all_data_train.isSignal == 0]['train_w']    \n",
    "    \n",
    "    weights_sig_test  = all_data_test[all_data_test.isSignal == 1]['train_w']\n",
    "    weights_bkg_test  = all_data_test[all_data_test.isSignal == 0]['train_w']\n",
    "\n",
    "    ## but before the splitting, so re-normalise\n",
    "    weights_sig_train = weights_sig_train.multiply(1./weights_sig_train.sum())\n",
    "    weights_bkg_train = weights_bkg_train.multiply(1./weights_bkg_train.sum())\n",
    "    \n",
    "    weights_sig_test = weights_sig_test.multiply(1./weights_sig_test.sum())\n",
    "    weights_bkg_test = weights_bkg_test.multiply(1./weights_bkg_test.sum())\n",
    "\n",
    "    #if n_classses > 2 sig proba is the last one (in the way the code is written) \n",
    "    Y_pred_sig_train = model.predict(sig_train)[:,0]\n",
    "    Y_pred_bkg_train = model.predict(bkg_train)[:,0]\n",
    "    Y_pred_sig_test  = model.predict(sig_test)[:,0]\n",
    "    Y_pred_bkg_test  = model.predict(bkg_test)[:,0]\n",
    "\n",
    "    # This will be the min/max of our plots\n",
    "    c_max = max(np.max(d) for d in np.concatenate([Y_pred_sig_train,Y_pred_bkg_train,Y_pred_sig_test,Y_pred_bkg_test]))\n",
    "    c_min = min(np.min(d) for d in np.concatenate([Y_pred_sig_train,Y_pred_bkg_train,Y_pred_sig_test,Y_pred_bkg_test]))\n",
    "\n",
    "    # Get histograms of the classifiers\n",
    "    Histo_training_S = np.histogram(Y_pred_sig_train,bins=40,range=(c_min,c_max),weights=weights_sig_train)\n",
    "    Histo_training_B = np.histogram(Y_pred_bkg_train,bins=40,range=(c_min,c_max),weights=weights_bkg_train)\n",
    "    Histo_testing_S = np.histogram(Y_pred_sig_test,bins=40,range=(c_min,c_max),weights=weights_sig_test)\n",
    "    Histo_testing_B = np.histogram(Y_pred_bkg_test,bins=40,range=(c_min,c_max),weights=weights_bkg_test)\n",
    "    \n",
    "    # Lets get the min/max of the Histograms\n",
    "    AllHistos = [Histo_training_S,Histo_training_B,Histo_testing_S,Histo_testing_B]\n",
    "    h_max     = max([histo[0].max() for histo in AllHistos])*1.2\n",
    "    h_min     = min([histo[0].min() for histo in AllHistos])\n",
    "    \n",
    "    # Get the histogram properties (binning, widths, centers)\n",
    "    bin_edges = Histo_training_S[1]\n",
    "    bin_centers = ( bin_edges[:-1] + bin_edges[1:]  ) /2.\n",
    "    bin_widths = (bin_edges[1:] - bin_edges[:-1])\n",
    "    \n",
    "    # To make error bar plots for the data, take the Poisson uncertainty sqrt(N)\n",
    "    ErrorBar_testing_S = np.sqrt(Histo_testing_S[0]/Y_pred_sig_test.size)\n",
    "    ErrorBar_testing_B = np.sqrt(Histo_testing_B[0]/Y_pred_bkg_test.size)\n",
    "    \n",
    "    plt.clf() \n",
    "    # Draw objects\n",
    "    ax1 = plt.subplot(111)\n",
    "    \n",
    "    # Draw solid histograms for the training data\n",
    "    ax1.bar(bin_centers-bin_widths/2.,Histo_training_S[0],facecolor='blue',linewidth=0,width=bin_widths,label='S (Train)',alpha=0.5)\n",
    "    ax1.bar(bin_centers-bin_widths/2.,Histo_training_B[0],facecolor='red',linewidth=0,width=bin_widths,label='B (Train)',alpha=0.5)\n",
    "    \n",
    "    # # Draw error-bar histograms for the testing data\n",
    "    ax1.errorbar(bin_centers, Histo_testing_S[0], yerr=ErrorBar_testing_S, xerr=None, ecolor='blue',c='blue',fmt='o',label='S (Test)')\n",
    "    ax1.errorbar(bin_centers, Histo_testing_B[0], yerr=ErrorBar_testing_B, xerr=None, ecolor='red',c='red',fmt='o',label='B (Test)')\n",
    "    \n",
    "    # Make a colorful backdrop to show the clasification regions in red and blue\n",
    "    ax1.axvspan(0.5, c_max, color='blue',alpha=0.08)\n",
    "    ax1.axvspan(c_min,0.5, color='red',alpha=0.08)\n",
    "\n",
    "    # Adjust the axis boundaries (just cosmetic)\n",
    "    ax1.axis([c_min, c_max, h_min, h_max])\n",
    "\n",
    "    # Make labels and title\n",
    "    plt.title(\"Classification with scikit-learn\")\n",
    "    plt.xlabel(\"Classifier output\")\n",
    "    plt.ylabel(\"Normalized Yields\")\n",
    "    \n",
    "    # Make legend with smalll font\n",
    "    legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
    "    for alabel in legend.get_texts():\n",
    "        alabel.set_fontsize('small')\n",
    "\n",
    "    # Save the result to png\n",
    "    plt.savefig(\"nn_score_output.png\")\n",
    "\n",
    "plot_classifier_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
